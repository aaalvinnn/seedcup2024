{
    "config_name": "PPO-16",
    "algorithm": "PPO",
    "actor_lr": 2e-5,
    "critic_lr": 5e-5,
    "num_episodes": 3000,
    "state_dim": 12,
    "actor_hidden_dim": 256,
    "critic_hidden_dim": 256,
    "action_dim": 6,
    "num_actor_hidden_layers": 5,
    "num_critic_hidden_layers": 5,
    "gamma": 0.99,
    "lmbda": 0.95,
    "residual_strength": 0,
    "epochs": 15,
    "eps": 0.2,

    "_score": "about 10~20 after 2000 episode"
}
