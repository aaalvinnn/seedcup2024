{
    "config_name": "PPO",
    "algorithm": "PPO",
    "actor_lr": 3e-5,
    "critic_lr": 1e-4,
    "num_episodes": 3000,
    "state_dim": 12,
    "actor_hidden_dim": 64,
    "critic_hidden_dim": 64,
    "action_dim": 6,
    "num_actor_hidden_layers": 6,
    "num_critic_hidden_layers": 6,
    "gamma": 0.99,
    "lmbda": 0.95,
    "residual_strength": 0,
    "epochs": 15,
    "eps": 0.2,
    "n_state_steps": 1,
    "a": 800,
    "b": 6,
    "c": 0.1,
    "d": 1,
    "e": 0.01
}
